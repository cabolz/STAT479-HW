---
title: "Week 11 Notes"
output: html_document
---

```{r setup, include=FALSE}
library("knitr")
opts_chunk$set(cache = TRUE, message = FALSE, warning = FALSE, echo = TRUE)
```

# Introduction to Topic Models

* Is a type of dimensionality reduction method that is especially useful for high-dimensional count matrices. 
* For example, it can be applied to,
    * Text data analysis, where each row is a document and each column is a word. The $ij^{th}$ entry contains the count of the $j^{th}$ word in the $i^{th}$ document.
    * Gene expression analysis, where each row is a biological sample and each column is a gene. The $ij^{th}$ entry measures the amount of gene $j$ expressed in sample $i$.
* For clarity, we will refer to samples as documents and features as words. However, keep in mind that these methods can be used more generally -- we will see a biological application three lectures from now.
* These models are useful to know about because they provide a compromise between clustering and PCA.
    * In clustering, each document would have to be assigned to a single topic.
    * In contrast, topic models allow each document to partially belong to several topics simultaneously. 
    * In this sense, they are more suitable when data do not belong to distinct, clearly-defined clusters.
    * PCA is also appropriate when the data vary continuously, but it does not provide any notion of clusters. In contrast, topic models estimate $K$ topics, which are analogous to a cluster centroids (though documents are typically a mix of several centroids).
* Without going into mathematical detail, topic models perform dimensionality
reduction by supposing,
    * Each document is a mixture of topics.
    * Each topic is a mixture of words.
  
```{r, fig.cap = "An overview of the topic modeling process. Topics are distributions over words, and the word counts of new documents are determined by their degree of membership over a set of underlying topics. In an ordinary clustering model, the bars for the memberships would have to be either pure purple or orange. Here, each document is a mixture.", preview = TRUE, echo = FALSE, out.width = 600, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/3shdh2f5vqarkwjucmmebigj2rwm4wyh.png")
```

* To illustrate the first point, consider modeling a collection of newspaper articles. A set of articles might belong primarily to the "politics" topic, and others to the "business" topic. Articles that describe a monetary policy in the federal reserve might belong partially to both the "politics" and the "business" topic.

* For the second point, consider the difference in words that would appear in politics and business articles. Articles about politics might frequently include words like "congress" and "law," but only rarely words like "stock" and "trade."

* Geometrically, LDA can be represented by the following picture. 
  * The corners of the simplex represent different words (in reality, there would be $V$ different corners to this simplex, one for each word). 
  * A simplex is the geometric object describing the set of probability vectors over $V$ elements. For example, if $V = 3$, then $\left(0.1,0, 0.9\right)$ and $\left(0.2, 0.3, 0.5\right)$ belong to the simplex, but not $\left(0.3, 0.1, 9\right)$, since it sums to a number larger than 1.
  * A topic is a point on this simplex. The closer the topic is to one of the corners, the more frequently that word appears in the topic.

```{r, fig.cap = "A geometric interpretation of LDA, from the original paper by Blei, Ng, and Jordan.", echo = FALSE, out.width = 400, fig.align = "center"}
include_graphics("http://2.bp.blogspot.com/-90BjNyRwkqk/T8bd9y7mUrI/AAAAAAAAAPg/H0Jdi-9RQ8s/s1600/LDA-f3.png")
```

* A document is a mixture of topics, with more words coming from the topics that it is close to.
  * More precisely, a document that is very close to a particular topic has a word distribution just like that topic. A document that is intermediate between two topics has a word distribution that mixes between both topics. 
  * Note that this is different from a clustering model, where all documents would lie at exactly one of the corners of the topic simplex. 
  * Finally, note that the topics form their own simplex, since each document can be described as a mixture of topics, with mixture weights summing up to 1.


***

# Fitting Topic Models

```{r}
library("dplyr")
library("ggplot2")
library("gutenbergr")
library("stringr")
library("tidyr")
library("tidytext")
library("topicmodels")
theme479 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    legend.position = "bottom"
  )
theme_set(theme479)
```

### Packages
* There are several packages in R that can be used to fit topic models
* We will use LDA as implemented in the `topicmodels` package,  which expects input to be structured as a `DocumentTermMatrix`, a special type of matrix that stores the counts of words (columns) across documents (rows).
* In practice, most of the effort required to fit a topic model goes into transforming the raw data into a suitable `DocumentTermMatrix`.

### Example
* To illustrate this process, let’s consider the "Great Library Heist" example from the reading
* We imagine that a thief has taken four books — Great Expectations, Twenty Thousand Leagues Under The Sea, War of the Worlds, and Pride & Prejudice — and torn all the chapters out. 
* We are left with pieces of isolated pieces of text and have to determine from which book they are from. 
* The block below downloads all the books into an R object.

```{r}
titles <- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")
books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")
books
```

* Since we imagine that the word distributions are not equal across the books, topic modeling is a reasonable approach for discovering the books associated with each chapter. 
* Note that, in principle, other clustering and dimensionality reduction procedures could also work.
* First, let’s simulate the process of tearing the chapters out. 
  * We split the raw texts anytime the word "Chapter" appears. 
  * We will keep track of the book names for each chapter, but this information is not passed into the topic modeling algorithm.

```{r}
by_chapter <- books %>%
  group_by(title) %>%
  mutate(
    chapter = cumsum(str_detect(text, regex("chapter", ignore_case = TRUE)))
  ) %>%
  group_by(title, chapter) %>%
  mutate(n = n()) %>%
  filter(n > 5) %>%
  ungroup() %>%
  unite(document, title, chapter)
```

* As it is, the text data are long character strings, giving actual text from the novels. 
* To fit LDA, we only need counts of each word within each chapter -- the algorithm throws away information related to word order. 
* To derive word counts
  * We first split the raw text into separate words using the `unest_tokens` function in the tidytext package. 
  * Then, we can count the number of times each word appeared in each document using `count`, a shortcut for the usual `group_by` and `summarize(n = n())` pattern.

```{r}
word_counts <- by_chapter %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(document, word) # shortcut for group_by(document, word) %>% summarise(n = n())
word_counts
```

* These words counts are still not in a format compatible with conversion to a `DocumentTermMatrix`. 
* The issue is that the `DocumentTermMatrix` expects words to be arranged along columns, but currently they are stored across rows. 
* The line below converts the original "long" word counts into a "wide" `DocumentTermMatrix` in one step.
* Across these 4 books, we have 65 chapters and a vocabulary of size 18325.

```{r}
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)
chapters_dtm
```

* Once the data are in this format, we can use the `LDA` function to fit a topic model. 
* We choose $K = 4$ topics because we expect that each topic will match a book. 
* Different hyperparameters can be set using the `control` argument.

```{r}
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda
```

* There are two types of outputs produced by the LDA model: the topic word distributions (for each topic, which words are common?) and the document-topic memberships (from which topics does a document come from?). 
* For visualization, it will be easiest to extract these parameters using the `tidy` function, specifying whether we want the topics (beta) or memberships (gamma).

```{r}
topics <- tidy(chapters_lda, matrix = "beta")
memberships <- tidy(chapters_lda, matrix = "gamma")
```

* This tidy approach is preferable to extracting the parameters directly from the fitted model (e.g., using `chapters_lda@gamma`) because it ensures the output is a tidy data.frame, rather than a matrix. 
* Tidy data.frames are easier to visualize using ggplot2.

```{r}
# highest weight words per topic
topics %>%
  arrange(topic, -beta)
# topic memberships per document
memberships %>%
  arrange(document, topic)
```

***

# Visualizing Topic Models

```{r}
library("dplyr")
library("forcats")
library("ggplot2")
library("ggrepel")
library("readr")
library("stringr")
library("superheat")
library("tibble")
library("tidyr")
library("tidytext")
library("topicmodels")
theme479 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    legend.position = "bottom"
  )
theme_set(theme479)
```

* In the last set of notes, we fit a topic model to the "Great Library Heist" dataset, but we did not visualize or interpret the results. We’ll work on that here. 
* The code below reads in the tidy topic and membership data.frames from before.

```{r}
memberships <- read_csv("https://uwmadison.box.com/shared/static/c5k5iinwo9au44fb3lc00vq6isbi72c5.csv")
topics <- read_csv("https://uwmadison.box.com/shared/static/uh34hhc1wnp072zcryisvgr3z0yh25ad.csv")
```

### Visualizing Topics

* A topic is a probability distribution across a collection of words. If the vocabulary isn’t too large, two appropriate visualization strategies are,
  	* Faceted barplot: Each facet corresponds to a topic. The height of each bar   	corresponds to a given word’s probability within the topic. The sum of heights across all bars is 1. 
  	* Heatmap: Each row is a topic and each column is a word. The color of the heatmap cells gives the probability of the word within the given topic.
	
* We can construct a faceted barplot using the tidied beta matrix
  * We've filtered to only words with a probability of at least $0.0003$ in at least one topic, but there are still more words than we could begin to inspect.
  * Nonetheless, it seems that there are words that have relatively high probability in one topic, but not others.

```{r, fig.heigt = 4, fig.cap = "A faceted barplot view of the original topic distributions, with only very limited filtering.", fig.align = "center"}
ggplot(topics %>% filter(beta > 3e-4), aes(term, beta)) +
  geom_col() +
  facet_grid(topic ~ .) +
  theme(axis.text.x = element_blank())
```

* For the heatmap, we need to pivot the topics, so that words appear along
columns. 
  * From there, we can use superheatmap.
  * The advantage of the heatmap is that it takes up less space, and while it obscures comparisons between word probabilities the main differences of interest are between low and high probability words.
  * Color is in general harder to compare than bar height.

```{r, fig.height = 2, fig.cap = "An equivalent heatmap view of the above faceted barplot.", fig.align = "center"}
topics %>%
  filter(beta > 3e-4) %>%
  pivot_wider(topic, term, values_from = "beta", values_fill = 0, names_repair = "unique") %>%
  select(-1) %>%
  superheat(
    pretty.order.cols = TRUE,
    legend = FALSE
  )
```

* Neither approach is very satisfactory since there are too many words for us to effectively label. A workaround is to restrict attention to a subset of "interesting" words. For example, we could filter to,
    * Top words overall: We can consider only words whose probabilities are above some threshold. This is the approach used in the visualizations above, though the threshold is very low (there are still too many words to add labels).
  	* Top words per topic: We can sort the words within each topic in order from highest to lowest probability, and then keep only the $S$ largest. 
  	* Most discriminative words: Some words have high probability just because they are common. They have high probability within each topic but aren't actually interesting as far as characterizing the topics is concerned. Instead, we can focus on words that are common in some topics but rare in others.
  	
* We can obtain the most probable words using the `slice_max` function, after first grouping by topic. 
  * Then, we use the same `reorder_within` function from the PCA lectures to reorder words within each topic. 
  * The resulting plot is much more interpretable.
  * Judging from the words that are common in each topic's distribution, we can guess that the topics approximately correspond to: 1 -> Great Expectations, 2 -> 20,000 Leagues Under the Sea, 3 -> Pride & Prejudice, 4 -> War of the Worlds.

```{r, preview = TRUE, fig.cap = "The top words associated with the four fitted topics from the Great Library Heist example.", fig.align = "center"}
top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  mutate(term = reorder_within(term, beta, topic))
ggplot(top_terms, aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "Set2") +
  scale_y_reordered()
```

* To visualize discriminative words, we first compute a discrimination measure for each word and filter to those with the top score. 
* The filtered results can be used in either faceted barplots or heatmaps. 
* Specifically, to find the words that discriminate between topics $k$ and $l$, we compute \begin{align*}D\left(k, l\right) := \beta_{kw}\log\left(\frac{\beta_{kw}}{\beta_{lw}}\right) + \left(\beta_{lw} - \beta_{kw}\right)\end{align*} for each word $w$. 
* By maximizing over all pairs $k, l$, we can determine whether the word is discriminative between any pair of topics. 
* This might seem like a mysterious formula, but it is just a function that is large when topic $k$ has much larger probability than topic $l$ (see the figure).

```{r}
kl_div <- function(p1, p2) {
  p1 * log(p1 / p2) + (p2 - p2)
}
kl_mat <- function(p) {
  K <- matrix(0, nrow = length(p), ncol = length(p))
  
  for (i in seq_along(p)) {
    for (j in seq_len(i - 1)) {
      K[i, j] <- kl_div(p[i], p[j])
    }
  }
  K
}
discrepancy <- function(p, lambda = 1e-7) {
  p <- (p + lambda) / sum(p + lambda) # Laplace smoothing
  K <- kl_mat(p)
  max(K)
}
```


```{r, fig.cap = "An illustration of the formula used for computing a word's discrimination between topics. The value of D is large when topic k has much larger probability than topic l.", fig.align = "center"}
p <- seq(0.01, .99, length.out = 50)
df <- expand.grid(p, p) %>%
  mutate(D = kl_div(Var1, Var2))
ggplot(df, aes(Var2, Var1)) +
  geom_tile(aes(col = D, fill = D)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_fixed() +
  scale_color_distiller(direction = 1) +
  scale_fill_distiller(direction = 1) +
  labs(
    y = expression(beta[kw]),
    x = expression(beta[lw])
  )
```

* An example heatmap of discriminative words is shown below. 
* This backs up our interpretation from the figure above. 
* It also has the advantage that it removes common words (e.g., hand, people, and time appeared in the plot above) and highlights rarer words that are specific to individual topics (e.g., names of characters that appear in only one of the books).

```{r, fig.height = 7, fig.width = 3, fig.cap = "A heatmap of the terms that are most discriminative across the four topics.", fig.align = "center"}
discriminative_terms <- topics %>%
  group_by(term) %>%
  mutate(D = discrepancy(beta)) %>%
  ungroup() %>%
  slice_max(D, n = 200) %>%
  mutate(term = fct_reorder(term, -D))
discriminative_terms %>%
  pivot_wider(term, topic, values_from = "beta") %>%
  column_to_rownames("term") %>%
  superheat(
    pretty.order.rows = TRUE,
    left.label.size = 1.5,
    left.label.text.size = 3,
    bottom.label.size = 0.05,
    legend = FALSE
  )
```

### Visualizing Memberships

* Besides the topics, it is useful to study the topic proportions for each chapter. 
* One compact approach is to use a boxplot. 
  * The result below suggest that each chapter is very definitely assigned to one of the four topics, except for chapters from Great Expectations. 
  * Therefore, while the model had the flexibility to learn more complex mixtures, it decided that a clustering structure made the most sense for Pride & Prejudice, War of the Worlds, and 20,000 Leagues Under the Sea.

```{r, fig.cap = "A boxplot of the document memberships. It seems that most documents are definitively assigned to one of the four topics.", fig.align = "center"}
memberships <- memberships %>%
  mutate(
    book = str_extract(document, "[^_]+"),
    topic = factor(topic)
  )
ggplot(memberships, aes(topic, gamma)) +
  geom_boxplot() +
  facet_wrap(~book)
```

* The boxplot considers the collection of documents in aggregate. 
* If we want to avoid aggregation and visualize individual documents, we can use a heatmap or jittered scatterplot. 
* These approaches are useful because heatmap cells and individual points can be drawn relatively small — anything requiring more space would become unwieldy as the number of documents grows. 
* For example, the plot below shows that chapter 119 of Great Expectations has unusually high membership in Topic 2 and low membership in topic 3.

```{r, fig.height = 4, fig.width = 6, fig.cap = "A jittered scatterplot of the topic memberships associated with each document.", fig.align = "center"}
ggplot(memberships, aes(topic, gamma, col = book)) +
  geom_point(position = position_jitter(h = 0.05, w = 0.3)) +
  geom_text_repel(aes(label = document), size = 3) +
  facet_wrap(~ book) +
  scale_color_brewer(palette = "Set1")
```

* Alternatively, we can use a "structure" plot. 
  * This is a type of stacked barplot where the colors of each bar corresponds to a topic.
  * We've sorted the documents using the result of a hierarchical clustering on their proportion vectors -- this is like how superheatmap orders rows using a dendrogram when using `pretty.order.rows`. 
  * The takeaways here are similar to those in the scatterplot above.

```{r, fig.height = 10, fig.width = 4, fig.cap = "A structure plot view of each chapter's topic memberships."}
gamma <- memberships %>%
  pivot_wider(document:book, topic, values_from = gamma)
```

```{r, fig.height = 10, fig.width = 4, fig.cap = "A structure plot view of each chapter's topic memberships.", fig.align = "center"}
hclust_result <- hclust(dist(gamma[, 3:6]))
document_order <- gamma$document[hclust_result$order]
memberships <- memberships %>%
  mutate(document = factor(document, levels = document_order))
ggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +
  geom_col(position = position_stack()) +
  facet_grid(book ~ ., scales = "free", space = "free") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_fill_brewer(palette = "Set2") +
  scale_color_brewer(palette = "Set2") +
  theme(axis.text.y = element_blank())
```

***

# Topic Modeling Case Study

```{r}
library("dplyr")
library("forcats")
library("ggplot2")
library("readr")
library("superheat")
library("tibble")
library("tidyr")
library("tidytext")
library("topicmodels")
theme479 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    legend.position = "bottom"
  )
theme_set(theme479)
```

* We have used text data analysis to motivate and illustrate the use of topic models. However, these models can be used whenever we have high-dimensional count data.
* In fact, topic models are an example of a larger family of models, called mixed-membership models. All of these models generalize clustering, and different variants can be applied to other data types, like continuous, categorical, and network data.
* To illustrate this broad applicability, this lecture will consider an example from gene expression analysis.
* The dataset we consider comes from the GTEX consortium. A variety of tissue
samples have been subject to RNA-seq analysis, which measures how much of each
type of gene is expressed within each sample. Intuitively, we relate,
  	* Documents &rarr; Tissue samples
  	* Words  &rarr; Genes
  	* Word Counts &rarr; Gene expression levels
	
```{r}
x <- read_csv("https://uwmadison.box.com/shared/static/fd437om519i5mrnur14xy6dq3ls0yqt2.csv")
x
```
	
* The goal here is to find sets of genes that tend to be expressed together, because these co-expression patterns might be indications of shared biological processes. 
* Unlike clustering, which assumes that each sample is described by one gene expression profile, a topic model will be able to model each tissue sample as a mixture of profiles (i.e., a mixture of underlying biological processes).

* As a first step in our analysis, we need to prepare a `DocumentTermMatrix` for use by the topicmodels package.
  * Since the data were in tidy format, we can use the `cast_dtm` function to spreaed genes across columns. 
  * From there, we can fit an LDA model. 
  * However, we've commented out the code (it takes a while to run) and instead just download the results that we've hosted on Box.

```{r}
x_dtm <- cast_dtm(x, sample, gene, value)
#fit <- LDA(x_dtm, k = 10, control = list(seed = 479))
#save(fit, file = "lda_gtex.rda")
f <- tempfile()
download.file("https://uwmadison.box.com/shared/static/ifgo6fbvm8bdlshzegb5ty8xif5istn8.rda", f)
fit <- get(load(f))
```

* Let's extract the tidy topic and memberships data. For the memberships, we will also join in the tissue from which each biological sample belonged.

```{r}
tissue_info <- x %>%
  select(sample, starts_with("tissue")) %>%
  unique()
topics <- tidy(fit, matrix = "beta") %>%
  mutate(topic = factor(topic))
memberships <- tidy(fit, matrix = "gamma") %>%
  mutate(topic = factor(topic)) %>%
  left_join(tissue_info, by = c("document" = "sample"))
```

* We can now visualize the topics. 
  * Let’s consider the genes with the highest discrimination between topics, using the same discrimination score as in the previous notes. 
  * Each row in the heatmap below is a gene, and each column is a topic. The intensity of color represents the gene's probability within the corresponding topic. 
  * Since only discriminative genes are shown, it's not surprising that most genes are only active within a subset of topics.

```{r, echo = FALSE}
# same formula from previous notes
kl_div <- function(p1, p2) {
  p1 * log(p1 / p2) + (p2 - p2)
}
kl_mat <- function(p) {
  K <- matrix(0, nrow = length(p), ncol = length(p))
  
  for (i in seq_along(p)) {
    for (j in seq_len(i - 1)) {
      K[i, j] <- kl_div(p[i], p[j])
    }
  }
  K
}
discrepancy <- function(p, lambda = 1e-7) {
  p <- (p + lambda) / sum(p + lambda) # Laplace smoothing
  K <- kl_mat(p)
  max(K)
}
```

```{r, fig.height = 8, fig.width = 4.5, fig.cap = "A heatmap of the most discriminative genes across the 10 estimated topics.", fig.align = "center"}
discriminative_genes <- topics %>%
  group_by(term) %>%
  mutate(D = discrepancy(beta)) %>%
  ungroup() %>%
  slice_max(D, n = 400) %>%
  mutate(term = fct_reorder(term, -D))
discriminative_genes %>%
  pivot_wider(term, topic, values_from = "beta") %>%
  column_to_rownames("term") %>%
  superheat(
    pretty.order.rows = TRUE,
    left.label.size = 1.5,
    left.label.text.size = 3,
    bottom.label.size = 0.05,
    legend = FALSE
  )
```

* Now, let’s see what tissues are related to which topics. 
* We can use a structure plot. Before making the plot, we prepare the data appropriately.
  * First, there are some tissues with very few samples, so we will filter those
tissues away. 
  * Second, we will reorder the samples so that those samples with similar topic profiles are placed next to one another. This is accomplished by running a hierarchical clustering on the topic membership vectors and extracting
the order of the resulting dendrogram leaves.

```{r}
keep_tissues <- memberships %>%
  count(tissue) %>%
  filter(n > 70) %>%
  pull(tissue)
hclust_result <- hclust(dist(fit@gamma))
document_order <- fit@documents[hclust_result$order]
memberships <- memberships %>%
  filter(tissue %in% keep_tissues) %>%
  mutate(document = factor(document, levels = document_order))
```

* Next, we can generate the structure plot. 
* The first three lines are the key lines: they create a stacked barchart for each sample and then facet across
tissues. 
* The remaining lines simply refine the appearance of the plot.

```{r, fig.height = 8, fig.width = 5, preview = TRUE, fig.cap = "A structure plot, showing the topic memberships across all tissue samples in the dataset.", fig.align = "center"}
ggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +
  geom_col(position = position_stack()) +
  facet_grid(tissue ~ ., scales = "free", space = "free") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_color_brewer(palette = "Set3", guide = FALSE) +
  scale_fill_brewer(palette = "Set3") +
  labs(x = "Topic Membership", y = "Sample", fill = "Topic") +
  theme(
    panel.spacing = unit(0.5, "lines"),
    strip.switch.pad.grid = unit(0, "cm"),
    strip.text.y = element_text(size = 8, angle = 0),
    axis.text.y = element_blank(),
  )
```

* From this plot, we can see clearly that different tissues express different combinations of topics
* For example, pancreas tissue typically expresses genes with high probability in topics 3 and 8. 
* Further, within tissues, there can be differences in the types of genes expressed -- certain blood cells are almost entirely summarized by topic 1, but most require some mixture of topics 1 and 6.
* Finally, we see that there are some topics that are common across several tissues. 
* For example, topic 4 is key component of thyroid, skin, muscle, lung, and some brain tissue.